{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import gpjax as gpx\n",
    "from gpjax.typing import Array, Float\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from jax import jacfwd, jacrev\n",
    "import optax as ox\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow interactive plots\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reproduce a 3D simulation study, using ideas in the [Linearly Constrained Gaussian Processes](https://papers.nips.cc/paper_files/paper/2017/hash/71ad16ad2c4d81f348082ff6c4b20768-Abstract.html) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Curl-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following latent function, which is the gradient of a scalar potential field given by.\n",
    "\n",
    "$$\\mathbf f:\\mathbb{R}^D\\to\\mathbb{R}^K$$\n",
    "\n",
    "where $D=K=3$, defined by:\n",
    "\n",
    "$$\\mathbf f(\\mathbf x) = - \\nabla P$$\n",
    "\n",
    "for:\n",
    "\n",
    "$$P = - e^{-a (xy+yz+zx)} \\sin{(xy+yz+zx)}$$\n",
    "\n",
    "with $a=0.01$\n",
    "\n",
    "Importantly, $\\boldsymbol \\nabla\\times \\mathbf f = 0$ see eg. [wikipedia](https://en.wikipedia.org/wiki/Vector_calculus_identities#Curl_of_gradient_is_zero). In this notebook, we will exploit this linear constraint to improve Gaussian Process Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curl_free_3d_example(x, y, z, a=0.01):\n",
    "    # gradient of the scalar potential: \n",
    "    # exp(-a (x*y + y*z + z*x)) * sin(x*y + y*z + z*x)\n",
    "    argument = x * y + y * z + z * x\n",
    "    exp_term = np.exp(-a * argument)\n",
    "    x_trig = (y + z) * np.cos(argument) - a * (y + z) * np.sin(argument)\n",
    "    y_trig = (x + z) * np.cos(argument) - a * (x + z) * np.sin(argument)\n",
    "    z_trig = (x + y) * np.cos(argument) - a * (x + y) * np.sin(argument)\n",
    "    return exp_term * x_trig, exp_term * y_trig, exp_term * z_trig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict our focus to the following region: $[0,1]\\times [0,1]\\times [0,1]$\n",
    "### Train points\n",
    "Choose 50 train points uniformly randomly from the region\n",
    "### Test points\n",
    "A grid of $n_{\\text{divisions}}^3$ points evenly spaced (in each dimension)\n",
    "\n",
    "### Processing data\n",
    "Initially the each datum is of the form $(\\mathbf x,\\mathbf y)$, where $\\mathbf x,\\mathbf y\\in\\mathbb{R}^3$.\n",
    "\n",
    "This is modified to three measurements: $\\{((\\mathbf x, i), y_i)\\}_{i=1}^3$. This ensures that the output is one dimensional, and so the kernel function is scalar valued. Explicitly, denoting the matrix-valued kernel function as $\\mathbf K$ and the scalar-valued kernel function as $\\tilde K$:\n",
    "\n",
    "$$K_{ij}(\\mathbf x,\\mathbf x') = \\tilde K((\\mathbf x,i),(\\mathbf x',j))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_divisions = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_position_3d(data, inducing=0.0):\n",
    "    # introduce alternating axis label\n",
    "    n_points = len(data[0])\n",
    "    axis_label = jnp.tile(jnp.array([0.0, 1.0, 2.0]), n_points)\n",
    "    axis_labeled_position = jnp.vstack(\n",
    "        (jnp.repeat(data, repeats=3, axis=1), axis_label)\n",
    "    ).T\n",
    "    # introduce label distinguishing between observations and inducing points\n",
    "    return jnp.concatenate(\n",
    "        (axis_labeled_position, inducing * jnp.ones((n_points * 3, 1))), axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "def stack_vector(data):\n",
    "    return data.T.flatten().reshape(-1, 1)\n",
    "\n",
    "\n",
    "def dataset_5d(pos, obs, inducing=0.0):\n",
    "    return gpx.Dataset(label_position_3d(pos, inducing), stack_vector(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = jnp.mgrid[\n",
    "    0 : 1 : n_divisions * 1j, 0 : 1 : n_divisions * 1j, 0 : 1 : n_divisions * 1j\n",
    "].reshape(3, -1)\n",
    "\n",
    "observations = jnp.stack(\n",
    "    curl_free_3d_example(positions[0], positions[1], positions[2]), axis=0\n",
    ").reshape(3, -1)\n",
    "\n",
    "dataset = dataset_5d(positions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_key = jr.PRNGKey(0)\n",
    "\n",
    "# Proposition for training data, unrestricted by a grid\n",
    "train_positions = jr.uniform(\n",
    "    key=simulation_key, minval=0., maxval=1., shape=(50, 3)\n",
    ").reshape(3, -1)\n",
    "\n",
    "train_observations = jnp.stack(\n",
    "    curl_free_3d_example(train_positions[0], train_positions[1], train_positions[2]),\n",
    "    axis=0,\n",
    ").reshape(3, -1)\n",
    "\n",
    "# Add noise to the observations, roughly 1% of the mean value\n",
    "noise = 1e-2\n",
    "train_observations += noise * jr.normal(simulation_key, train_observations.shape)\n",
    "\n",
    "training_data = dataset_5d(train_positions, train_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth + measurements plot\n",
    "\n",
    "# Generate a grid of points\n",
    "x = np.linspace(0, 1, 8)\n",
    "y = np.linspace(0, 1, 8)\n",
    "# only 3 points in z direction to avoid clutter\n",
    "z = np.linspace(0, 1, 3)\n",
    "X, Y, Z = np.meshgrid(x, y, z)\n",
    "\n",
    "# Compute the vector field components\n",
    "U, V, W = curl_free_3d_example(X, Y, Z)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "fig.canvas.toolbar_visible = False\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.quiver(X, Y, Z, U, V, W, length=0.15, normalize=False, label='ground truth', linewidth=0.5)\n",
    "ax.quiver(\n",
    "    training_data.X[::3][:, 0], \n",
    "    training_data.X[::3][:, 1], \n",
    "    training_data.X[::3][:, 2], \n",
    "    training_data.y[::3].squeeze(), \n",
    "    training_data.y[1::3].squeeze(), \n",
    "    training_data.y[2::3].squeeze(), \n",
    "    length=0.15, \n",
    "    normalize=False,\n",
    "    label='measurements',\n",
    "    linewidth=1.,\n",
    "    color='r')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "fig.legend(ncol=2, loc='lower center')\n",
    "\n",
    "plt.subplots_adjust(left=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal kernel\n",
    "The diagonal kernel is where \n",
    "$$K_{ij}(\\mathbf x,\\mathbf x') = \\tilde K((\\mathbf x,i),(\\mathbf x',j)) = \\delta_{ij}k(\\mathbf x,\\mathbf x')$$\n",
    "\n",
    "for some $k(\\mathbf x,\\mathbf x')$.\n",
    "\n",
    "Therefore, the outputs of the predicted function are independent. This is equivalent to totally separating the dataset into three different datasets: $\\mathcal D_i:=\\{(\\mathbf x_n, y^{(i)}_n)\\}_{n=1}^{N}$ for $i=1,2,3$ and performing a Gaussian Process Regression on each dataset separately.\n",
    "\n",
    "This does not use any prior information about the divergence-free nature of the underlying latent function, so is expected to perform worse than other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VectorKernel_3d(gpx.kernels.AbstractKernel):\n",
    "    kernel: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1, 2])\n",
    "\n",
    "    def __call__(\n",
    "        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        # standard RBF-SE kernel if x and x' are on the same output, otherwise returns 0\n",
    "\n",
    "        w = jnp.array(X[3], dtype=int)\n",
    "        wp = jnp.array(Xp[3], dtype=int)\n",
    "\n",
    "        # drop output label to reduce resource usage\n",
    "        X = jnp.array(X[0:3])\n",
    "        Xp = jnp.array(Xp[0:3])\n",
    "\n",
    "        K = (w == wp) * self.kernel(X, Xp)\n",
    "\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPJax implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_gp(kernel, mean, dataset):\n",
    "    prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n",
    "    likelihood = gpx.likelihoods.Gaussian(\n",
    "        num_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)\n",
    "    )\n",
    "    posterior = prior * likelihood\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = gpx.mean_functions.Zero()\n",
    "kernel = VectorKernel_3d()\n",
    "diagonal_posterior = initialise_gp(kernel, mean, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_mll(posterior, dataset, NIters=200):\n",
    "    # define the MLL using dataset_train\n",
    "    objective = gpx.objectives.ConjugateMLL(negative=True)\n",
    "    # Optimise to minimise the MLL\n",
    "    \n",
    "    # use Adam optimiser for improved robustness\n",
    "    optimiser = ox.adam(learning_rate=0.1)\n",
    "    \n",
    "    opt_posterior, _ = gpx.fit(\n",
    "        model=posterior,\n",
    "        objective=objective,\n",
    "        optim=optimiser,\n",
    "        train_data=dataset,\n",
    "        num_iters=NIters,\n",
    "        key=jr.PRNGKey(0),\n",
    "    )\n",
    "    return opt_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_diagonal_posterior = optimise_mll(diagonal_posterior, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are evaluated by calculating the RMSE between the predicted and true outputs. In particular,\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{N_P D}\\sum_{i=1}^{N_PD} \\|\\mathbf y_p-\\mathbf y_t\\|_2^2}$$\n",
    "\n",
    "where $\\mathbf y_p$ are the predicted values and $\\mathbf y_t$ are the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, truth):\n",
    "    # in the paper they compute RMS per vector component\n",
    "    return jnp.sqrt(jnp.sum((predictions - truth) ** 2) / truth.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_distribution(opt_posterior, prediction_locations, dataset_train):\n",
    "    latent = opt_posterior.predict(prediction_locations, train_data=dataset_train)\n",
    "    latent_mean = latent.mean()\n",
    "    latent_std = latent.stddev()\n",
    "    return latent_mean, latent_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal_mean, diagonal_std = latent_distribution(\n",
    "    opt_diagonal_posterior, dataset.X, training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_diagonal = dataset_5d(positions, diagonal_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(dataset_latent_diagonal.y, dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth + diagonal estimate + residuals plot\n",
    "def field_comparison_plots(ground_truth, measurements, gp_estimate):\n",
    "    fig = plt.figure(figsize=(11, 4))\n",
    "    \n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "    \n",
    "    ax0 = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "    ax0.set_title('Ground Truth')\n",
    "    \n",
    "    ax0.quiver(\n",
    "        ground_truth.X[::3][:, 0], \n",
    "        ground_truth.X[::3][:, 1], \n",
    "        ground_truth.X[::3][:, 2], \n",
    "        ground_truth.y[::3].squeeze(), \n",
    "        ground_truth.y[1::3].squeeze(), \n",
    "        ground_truth.y[2::3].squeeze(), \n",
    "        length=0.15, \n",
    "        normalize=False,\n",
    "        label='Field value',\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    \n",
    "    ax0.quiver(\n",
    "        measurements.X[::3][:, 0], \n",
    "        measurements.X[::3][:, 1], \n",
    "        measurements.X[::3][:, 2], \n",
    "        measurements.y[::3].squeeze(), \n",
    "        measurements.y[1::3].squeeze(), \n",
    "        measurements.y[2::3].squeeze(), \n",
    "        length=0.15, \n",
    "        normalize=False,\n",
    "        label='measurements',\n",
    "        linewidth=1.,\n",
    "        color='r',\n",
    "    )\n",
    "    \n",
    "    ax1 = fig.add_subplot(132, projection='3d')\n",
    "\n",
    "    ax1.set_title('GP Estimate')\n",
    "    \n",
    "    ax1.quiver(\n",
    "        gp_estimate.X[::3][:, 0], \n",
    "        gp_estimate.X[::3][:, 1], \n",
    "        gp_estimate.X[::3][:, 2], \n",
    "        gp_estimate.y[::3].squeeze(), \n",
    "        gp_estimate.y[1::3].squeeze(), \n",
    "        gp_estimate.y[2::3].squeeze(), \n",
    "        length=0.15,\n",
    "        normalize=False,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    \n",
    "    ax1.quiver(\n",
    "        measurements.X[::3][:, 0], \n",
    "        measurements.X[::3][:, 1], \n",
    "        measurements.X[::3][:, 2], \n",
    "        measurements.y[::3].squeeze(), \n",
    "        measurements.y[1::3].squeeze(), \n",
    "        measurements.y[2::3].squeeze(), \n",
    "        length=0.15, \n",
    "        normalize=False,\n",
    "        linewidth=1.,\n",
    "        color='r',\n",
    "    )\n",
    "    \n",
    "    ax2 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "    ax2.set_title('Residuals')\n",
    "    \n",
    "    residuals = ground_truth.y - gp_estimate.y\n",
    "    \n",
    "    ax2.quiver(\n",
    "        ground_truth.X[::3][:, 0], \n",
    "        ground_truth.X[::3][:, 1], \n",
    "        ground_truth.X[::3][:, 2], \n",
    "        residuals[::3].squeeze(), \n",
    "        residuals[1::3].squeeze(), \n",
    "        residuals[2::3].squeeze(), \n",
    "        length=0.15, \n",
    "        normalize=False,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    \n",
    "    ax2.quiver(\n",
    "        measurements.X[::3][:, 0], \n",
    "        measurements.X[::3][:, 1], \n",
    "        measurements.X[::3][:, 2], \n",
    "        measurements.y[::3].squeeze(), \n",
    "        measurements.y[1::3].squeeze(), \n",
    "        measurements.y[2::3].squeeze(), \n",
    "        length=0.15, \n",
    "        normalize=False,\n",
    "        linewidth=1.,\n",
    "        color='r',\n",
    "    )\n",
    "\n",
    "    for ax_ in [ax0, ax1, ax2]:\n",
    "        ax_.set_xlabel('x')\n",
    "        ax_.set_ylabel('y')\n",
    "\n",
    "    fig.legend(ncol=2, loc='lower center')\n",
    "    \n",
    "    plt.subplots_adjust(left=0)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_comparison_plots(dataset, training_data, dataset_latent_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curl free kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of curl free kernel\n",
    "The latent function $\\mathbf f$ was chosen such that it is curl-free, in other words:\n",
    "$$\\boldsymbol \\nabla\\times \\mathbf f := \\begin{pmatrix}\\dfrac{\\partial f_3}{\\partial x_2} - \\dfrac{\\partial f_2}{\\partial x_3}\\\\\n",
    "\\dfrac{\\partial  f_1}{\\partial x_3} - \\dfrac{\\partial  f_3}{\\partial x_1}\\\\\n",
    "\\dfrac{\\partial  f_2}{\\partial x_1}-\\dfrac{\\partial  f_1}{\\partial x_2}\\end{pmatrix}=\\mathbf 0$$\n",
    "\n",
    "Note that*, given any differentiable function $g$, the function:\n",
    "$$\\mathbf f(\\mathbf x):= \\mathscr G_\\mathbf x g := \\nabla _{\\mathbf x} g$$\n",
    "automatically satisfies the required constraint.\n",
    "\n",
    "As is the case with multivariate Gaussians, linear transformations of GPs are GPs (and they transform in much the same way).\n",
    "In particular:\n",
    "$$g \\sim \\mathcal{GP}(0, k_g) \\implies \\mathscr G_\\mathbf x g \\sim \\mathcal{GP}(\\mathbf 0, \\mathscr G_\\mathbf x k_g \\mathscr G_{\\mathbf x'}^\\top)$$\n",
    "\n",
    "In our case, we choose $k_g$ to be the squared exponential kernel. Therefore, any $\\mathbf f$ picked from the distribution $\\mathbf f \\sim \\mathcal {GP}(\\mathbf 0, \\mathscr G_\\mathbf x  k_g(\\mathbf x, \\mathbf x') \\mathscr G_{\\mathbf x'}^\\top)$, where\n",
    "$$\\mathscr G_\\mathbf x  k_g(\\mathbf x, \\mathbf x') \\mathscr G_{\\mathbf x'}^\\top= \\begin{pmatrix}\\dfrac{\\partial^2}{\\partial x_1 x_1'} & \\dfrac{\\partial^2}{\\partial x_1 x_2'} & \\dfrac{\\partial^2}{\\partial x_1 x_3'}\\\\\n",
    "\\dfrac{\\partial^2}{\\partial x_2 x_1'} & \\dfrac{\\partial^2}{\\partial x_2 x_2'} & \\dfrac{\\partial^2}{\\partial x_2 x_3'}\\\\\n",
    "\\dfrac{\\partial^2}{\\partial x_3 x_1'} & \\dfrac{\\partial^2}{\\partial x_3 x_2'} & \\dfrac{\\partial^2}{\\partial x_3 x_3'}\n",
    "\\end{pmatrix}k_g(\\mathbf x, \\mathbf x')$$\n",
    "will satisfy the required constraint. This is the divergence-free kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Much of the paper is dedicated to devising a systematic way to construct $\\mathscr{G}_\\mathbf x$ for arbitrary linear constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_hessian(\n",
    "    kernel, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    ") -> Float[Array, \"1\"]:\n",
    "    # compute all relevant second derivatives at once\n",
    "    # eg small_hessian(k)[0][1] is d2k/dx1dy2\n",
    "    return jnp.array(\n",
    "        jacfwd(jacrev(kernel, argnums=0), argnums=1)(X, Xp), dtype=jnp.float64\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CurlFreeKernel(gpx.kernels.AbstractKernel):\n",
    "    kernel: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1, 2])\n",
    "\n",
    "    def __call__(\n",
    "        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        axis_1 = jnp.array(X[3], dtype=int)\n",
    "        axis_2 = jnp.array(Xp[3], dtype=int)\n",
    "\n",
    "        # drop output label to reduce resource usage\n",
    "        X = jnp.array(X[0:3])\n",
    "        Xp = jnp.array(Xp[0:3])\n",
    "\n",
    "        hess = small_hessian(self.kernel, X, Xp)\n",
    "\n",
    "        K = hess[axis_1][axis_2]\n",
    "\n",
    "        return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = CurlFreeKernel()\n",
    "curl_free_posterior = initialise_gp(kernel, mean, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_curl_free_posterior = optimise_mll(curl_free_posterior, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl_free_mean, curl_free_std = latent_distribution(\n",
    "    opt_curl_free_posterior, dataset.X, training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_curl_free = dataset_5d(positions, curl_free_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(dataset_latent_curl_free.y, dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_comparison_plots(dataset, training_data, dataset_latent_curl_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPD (Negative Log Predictive Density)\n",
    "An alternative to RMSE for measuring how well the predicted model matches the true values. It is formally the log-likelihood of predicting the true values using the model. It is calculated using the following formula:\n",
    "\n",
    "$$\\text{NLPD} = -\\sum_{i=1}^{N_P} \\log p(\\mathbf y_i|\\mathbf x_i)$$\n",
    "\n",
    "Here $p(\\mathbf y|\\mathbf x)$ is a Gaussian distribution with mean given by the posterior mean and standard deviation given by the posterior standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure testing data alternates between x0 and x1 components\n",
    "def nlpd(mean, std, true_observations):\n",
    "    test_grid = jnp.column_stack(\n",
    "        (true_observations[0], true_observations[1], true_observations[2])\n",
    "    ).flatten()\n",
    "    normal = tfp.substrates.jax.distributions.Normal(loc=mean, scale=std)\n",
    "    return -jnp.sum(normal.log_prob(test_grid))\n",
    "\n",
    "\n",
    "# compute nlpd for velocity and helmholtz\n",
    "nlpd_diagonal = nlpd(diagonal_mean, diagonal_std, observations)\n",
    "nlpd_curl_free = nlpd(curl_free_mean, curl_free_std, observations)\n",
    "\n",
    "print(\n",
    "    f\"NLPD for diagonal: {nlpd_diagonal:.2E} \\nNLPD for curl free: {nlpd_curl_free:.2E}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
