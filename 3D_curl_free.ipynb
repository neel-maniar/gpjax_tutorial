{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neelm\\miniconda3\\envs\\gp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import gpjax as gpx\n",
    "from gpjax.typing import Array, Float\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Curl-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following latent function, which is the field of an electric dipole.\n",
    "\n",
    "$$\\mathbf f:\\mathbb{R}^D\\to\\mathbb{R}^K$$\n",
    "\n",
    "where $D=K=3$, defined by:\n",
    "\n",
    "$$\\mathbf f(\\mathbf x) = \\dfrac 1 {4\\pi\\varepsilon_0}\\left(\\dfrac{3\\mathbf p\\cdot \\mathbf x}{r^5}\\mathbf x - \\dfrac{\\mathbf p}{r^3}\\right)$$\n",
    "\n",
    "with $\\varepsilon_0=1$ and $\\mathbf p = (0,0,1)^\\top$. The derivation of this may be found online easily, e.g. on [Wikipedia](https://en.wikipedia.org/wiki/Electric_dipole_moment#Potential_and_field_of_an_electric_dipole) or in any elementary electromagnetism textbook, for example the end of section 4.1 in _Classical Electrodynamics_ by Jackson.\n",
    "\n",
    "Importantly, $\\boldsymbol \\nabla\\times \\mathbf f = 0$, and in this notebook, we will exploit this linear constraint to improve Gaussian Process Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electric dipole field\n",
    "def curl_free_3d_example(x, y, z):\n",
    "    # assumes epsilon is 1\n",
    "    # dipole moment\n",
    "    p_x, p_y, p_z = 0, 0, 1\n",
    "\n",
    "    r = jnp.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "    e_term = 3 * (p_x * x + p_y * y + p_z * z) / r**5\n",
    "    p_term = 1 / r**3\n",
    "\n",
    "    f_x = (x * e_term - p_x * p_term) / (4 * jnp.pi)\n",
    "    f_y = (y * e_term - p_y * p_term) / (4 * jnp.pi)\n",
    "    f_z = (z * e_term - p_z * p_term) / (4 * jnp.pi)\n",
    "\n",
    "    return f_x, f_y, f_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict our focus to the following region: $[-2,2]\\times [-2,2]\\times [0.5,4.5]$\n",
    "### Train points\n",
    "Choose 50 train points uniformly randomly from the region\n",
    "### Test points\n",
    "A grid of $n_{\\text{divisions}}^3$ points evenly spaced (in each dimension)\n",
    "\n",
    "### Processing data\n",
    "Initially the each datum is of the form $(\\mathbf x,\\mathbf y)$, where $\\mathbf x,\\mathbf y\\in\\mathbb{R}^3$.\n",
    "\n",
    "This is modified to three measurements: $\\{((\\mathbf x, i), y_i)\\}_{i=1}^3$. This ensures that the output is one dimensional, and so the kernel function is scalar valued. Explicitly, denoting the matrix-valued kernel function as $\\mathbf K$ and the scalar-valued kernel function as $\\tilde K$:\n",
    "\n",
    "$$K_{ij}(\\mathbf x,\\mathbf x') = \\tilde K((\\mathbf x,i),(\\mathbf x',j))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_divisions = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_position_3d(data, inducing=0.0):\n",
    "    # introduce alternating axis label\n",
    "    n_points = len(data[0])\n",
    "    axis_label = jnp.tile(jnp.array([0.0, 1.0, 2.0]), n_points)\n",
    "    axis_labeled_position = jnp.vstack(\n",
    "        (jnp.repeat(data, repeats=3, axis=1), axis_label)\n",
    "    ).T\n",
    "    # introduce label distinguishing between observations and inducing points\n",
    "    return jnp.concatenate(\n",
    "        (axis_labeled_position, inducing * jnp.ones((n_points * 3, 1))), axis=1\n",
    "    )\n",
    "\n",
    "def stack_vector(data):\n",
    "    return data.T.flatten().reshape(-1, 1)\n",
    "\n",
    "def dataset_5d(pos, obs, inducing=0.0):\n",
    "    return gpx.Dataset(label_position_3d(pos, inducing), stack_vector(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = jnp.mgrid[-2:2:n_divisions*1j, -2:2:n_divisions*1j, 0.5:4.5:n_divisions*1j].reshape(3, -1)\n",
    "\n",
    "observations = jnp.stack(\n",
    "    curl_free_3d_example(positions[0], positions[1], positions[2]), axis=0\n",
    ").reshape(3, -1)\n",
    "\n",
    "dataset = dataset_5d(positions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_key = jr.PRNGKey(0)\n",
    "\n",
    "# Proposition for training data, unrestricted by a grid\n",
    "train_positions = jr.uniform(key=simulation_key, minval=0.0, maxval=4.0, shape=(50, 3)).reshape(3,-1) + jnp.array([-2., -2., 0.5]).reshape(3,1)\n",
    "train_observations = jnp.stack(\n",
    "    curl_free_3d_example(train_positions[0], train_positions[1], train_positions[2]), axis=0\n",
    ").reshape(3, -1)\n",
    "training_data = dataset_5d(train_positions, train_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal kernel\n",
    "The diagonal kernel is where \n",
    "$$K_{ij}(\\mathbf x,\\mathbf x') = \\tilde K((\\mathbf x,i),(\\mathbf x',j)) = \\delta_{ij}k(\\mathbf x,\\mathbf x')$$\n",
    "\n",
    "for some $k(\\mathbf x,\\mathbf x')$.\n",
    "\n",
    "Therefore, the outputs of the predicted function are independent. This is equivalent to totally separating the dataset into three different datasets: $\\mathcal D_i:=\\{(\\mathbf x_n, y^{(i)}_n)\\}_{n=1}^{N}$ for $i=1,2,3$ and performing a Gaussian Process Regression on each dataset separately.\n",
    "\n",
    "This does not use any prior information about the divergence-free nature of the underlying latent function, so is expected to perform worse than other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VectorKernel_3d(gpx.kernels.AbstractKernel):\n",
    "    kernel: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1, 2])\n",
    "\n",
    "    def __call__(\n",
    "        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        # standard RBF-SE kernel if x and x' are on the same output, otherwise returns 0\n",
    "\n",
    "        w = jnp.array(X[3], dtype=int)\n",
    "        wp = jnp.array(Xp[3], dtype=int)\n",
    "\n",
    "        # drop output label to reduce resource usage\n",
    "        X = jnp.array(X[0:3])\n",
    "        Xp = jnp.array(Xp[0:3])\n",
    "\n",
    "        K = (w == wp) * self.kernel(X, Xp)\n",
    "\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPJax implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_gp(kernel, mean, dataset):\n",
    "    prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n",
    "    likelihood = gpx.likelihoods.Gaussian(\n",
    "        num_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)\n",
    "    )\n",
    "    posterior = prior * likelihood\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = gpx.mean_functions.Zero()\n",
    "kernel = VectorKernel_3d()\n",
    "diagonal_posterior = initialise_gp(kernel, mean, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_mll(posterior, dataset, NIters=1000):\n",
    "    # define the MLL using dataset_train\n",
    "    objective = gpx.objectives.ConjugateMLL(negative=True)\n",
    "    # Optimise to minimise the MLL\n",
    "    opt_posterior, _ = gpx.fit_scipy(\n",
    "        model=posterior,\n",
    "        objective=objective,\n",
    "        train_data=dataset,\n",
    "        max_iters=NIters,\n",
    "    )\n",
    "    return opt_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_diagonal_posterior = optimise_mll(diagonal_posterior, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are evaluated by calculating the RMSE between the predicted and true outputs. In particular,\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{N_P D}\\sum_{i=1}^{N_P} \\|\\mathbf y_p-\\mathbf y_t\\|_2^2}$$\n",
    "\n",
    "where $\\mathbf y_p$ are the predicted values and $\\mathbf y_t$ are the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, truth):\n",
    "    # in the paper they compute RMS per vectror component\n",
    "    return jnp.sqrt(jnp.sum((predictions - truth) ** 2) / truth.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_distribution(opt_posterior, prediction_locations, dataset_train):\n",
    "    latent = opt_posterior.predict(prediction_locations, train_data=dataset_train)\n",
    "    latent_mean = latent.mean()\n",
    "    latent_std = latent.stddev()\n",
    "    return latent_mean, latent_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal_mean, diagonal_std = latent_distribution(\n",
    "    opt_diagonal_posterior, dataset.X, training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_diagonal = dataset_5d(positions, diagonal_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(dataset_latent_diagonal.y, dataset.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curl free kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of curl free kernel\n",
    "The latent function $\\mathbf f$ was chosen such that it is curl-free, in other words:\n",
    "$$\\boldsymbol \\nabla\\times \\mathbf f := \\begin{pmatrix}\\dfrac{\\partial f_3}{\\partial x_2} - \\dfrac{\\partial f_2}{\\partial x_3}\\\\\n",
    "\\dfrac{\\partial  f_1}{\\partial x_3} - \\dfrac{\\partial  f_3}{\\partial x_1}\\\\\n",
    "\\dfrac{\\partial  f_2}{\\partial x_1}-\\dfrac{\\partial  f_1}{\\partial x_2}\\end{pmatrix}=\\mathbf 0$$\n",
    "\n",
    "Note that*, given any differentiable function $g$, the function:\n",
    "$$\\mathbf f(\\mathbf x):= \\mathscr G_\\mathbf x g := \\nabla _{\\mathbf x} g$$\n",
    "automatically satisfies the required constraint.\n",
    "\n",
    "As is the case with multivariate Gaussians, linear transformations of GPs are GPs (and they transform in much the same way).\n",
    "In particular:\n",
    "$$g \\sim \\mathcal{GP}(0, k_g) \\implies \\mathscr G_\\mathbf x g \\sim \\mathcal{GP}(\\mathbf 0, \\mathscr G_\\mathbf x k_g \\mathscr G_{\\mathbf x'}^\\top)$$\n",
    "\n",
    "In our case, we choose $k_g$ to be the squared exponential kernel. Therefore, any $\\mathbf f$ picked from the distribution $\\mathbf f \\sim \\mathcal {GP}(\\mathbf 0, \\mathscr G_\\mathbf x  k_g(\\mathbf x, \\mathbf x') \\mathscr G_{\\mathbf x'}^\\top)$, where\n",
    "$$\\mathscr G_\\mathbf x  k_g(\\mathbf x, \\mathbf x') \\mathscr G_{\\mathbf x'}^\\top= \\begin{pmatrix}\\dfrac{\\partial^2}{\\partial x_1 x_1'} & \\dfrac{\\partial^2}{\\partial x_1 x_2'} & \\dfrac{\\partial^2}{\\partial x_1 x_3'}\\\\\n",
    "\\dfrac{\\partial^2}{\\partial x_2 x_1'} & \\dfrac{\\partial^2}{\\partial x_2 x_2'} & \\dfrac{\\partial^2}{\\partial x_2 x_3'}\\\\\n",
    "\\dfrac{\\partial^2}{\\partial x_3 x_1'} & \\dfrac{\\partial^2}{\\partial x_3 x_2'} & \\dfrac{\\partial^2}{\\partial x_3 x_3'}\n",
    "\\end{pmatrix}k_g(\\mathbf x, \\mathbf x')$$\n",
    "will satisfy the required constraint. This is the divergence-free kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Much of the paper is dedicated to devising a systematic way to construct $\\mathscr{G}_\\mathbf x$ for arbitrary linear constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_hessian(\n",
    "    kernel, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    ") -> Float[Array, \"1\"]:\n",
    "    # compute all relevant second derivatives at once\n",
    "    # eg small_hessian(k)[0][1] is d2k/dx1dy2\n",
    "    return jnp.array(\n",
    "        jacfwd(jacrev(kernel, argnums=0), argnums=1)(X, Xp), dtype=jnp.float64\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class CurlFreeKernel(gpx.kernels.AbstractKernel):\n",
    "    kernel: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1, 2])\n",
    "\n",
    "    def __call__(\n",
    "        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        axis_1 = jnp.array(X[3], dtype=int)\n",
    "        axis_2 = jnp.array(Xp[3], dtype=int)\n",
    "\n",
    "        # drop output label to reduce resource usage\n",
    "        X = jnp.array(X[0:3])\n",
    "        Xp = jnp.array(Xp[0:3])\n",
    "\n",
    "        hess = small_hessian(self.kernel, X, Xp)\n",
    "\n",
    "        K = hess[axis_1][axis_2]\n",
    "\n",
    "        return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = CurlFreeKernel()\n",
    "curl_free_posterior = initialise_gp(kernel, mean, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_curl_free_posterior = optimise_mll(curl_free_posterior, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl_free_mean, curl_free_std = latent_distribution(\n",
    "    opt_curl_free_posterior, dataset.X, training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_curl_free = dataset_5d(positions, curl_free_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(dataset_latent_curl_free.y, dataset.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPD (Negative Log Predictive Density)\n",
    "An alternative to RMSE for measuring how well the predicted model matches the true values. It is formally the log-likelihood of predicting the true values using the model. It is calculated using the following formula:\n",
    "\n",
    "$$\\text{NLPD} = -\\sum_{i=1}^{N_PD} \\log p(\\mathbf f(\\mathbf x_i)|\\mathbf x_i)$$\n",
    "\n",
    "Here $p(\\mathbf y|\\mathbf x)$ is a Gaussian distribution with mean given by the posterior mean and standard deviation given by the posterior standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure testing data alternates between x0 and x1 components\n",
    "def nlpd(mean, std, true_observations):\n",
    "    test_grid = jnp.column_stack((true_observations[0], true_observations[1], true_observations[2])).flatten()\n",
    "    normal = tfp.substrates.jax.distributions.Normal(loc=mean, scale=std)\n",
    "    return -jnp.sum(normal.log_prob(test_grid))\n",
    "\n",
    "\n",
    "# compute nlpd for velocity and helmholtz\n",
    "nlpd_diagonal = nlpd(diagonal_mean, diagonal_std, observations)\n",
    "nlpd_curl_free = nlpd(curl_free_mean, curl_free_std, observations)\n",
    "\n",
    "print(f\"NLPD for diagonal: {nlpd_diagonal:.2E} \\nNLPD for curl free: {nlpd_curl_free:.2E}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
