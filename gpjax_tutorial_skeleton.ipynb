{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import gpjax as gpx\n",
    "from gpjax.typing import Array, Float\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reproduce the 2D simulation study, as in the [Linearly Constrained Gaussian Processes](https://papers.nips.cc/paper_files/paper/2017/hash/71ad16ad2c4d81f348082ff6c4b20768-Abstract.html) paper.\n",
    "\n",
    "The data is a noisy measurement of a 2D divergence-free vector field.\n",
    "\n",
    "Two different kernels are used:\n",
    "- Diagonal kernel (NaÃ¯ve kernel, using no information about the dataset being divergence free)\n",
    "- Divergence-free kernel (Enforces the divergence-free condition globally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D div-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3rd dimension of 0 (x), 1 (y), 2 (0 div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with a simulated dataset $\\mathcal{D} = \\{(\\mathbf x_i, \\mathbf y_i)\\}_{i=1}^{50}$ with inputs $\\mathbf{x}$\n",
    "sampled uniformly on $[0, 4]\\times [0,4]$ and corresponding independent noisy outputs $\\mathbf y = \\mathbf f(\\mathbf x)+\\boldsymbol \\varepsilon$, defined by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& f_1\\left(x_1, x_2\\right)=e^{-a x_1 x_2}\\left(a x_1 \\sin \\left(x_1 x_2\\right)-x_1 \\cos \\left(x_1 x_2\\right)\\right) \\\\\n",
    "& f_2\\left(x_1, x_2\\right)=e^{-a x_1 x_2}\\left(x_2 \\cos \\left(x_1 x_2\\right)-a x_2 \\sin \\left(x_1 x_2\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and $\\boldsymbol \\varepsilon \\sim \\mathcal N(\\mathbf 0,\\sigma^2 \\mathbf I)$\n",
    "\n",
    "Here, we take $a=10^{-2}, \\sigma =10^{-4}$\n",
    "\n",
    "The test dataset is a uniform $20\\times 20$ grid on $[0,4]\\times [0,4]$ and noiseless outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_free_2d_example(x, y, a=0.01):\n",
    "    exp_term = jnp.exp(-a * x * y)\n",
    "    trig_term_x = a * x * jnp.sin(x * y) - x * jnp.cos(x * y)\n",
    "    trig_term_y = -a * y * jnp.sin(x * y) + y * jnp.cos(x * y)\n",
    "    return exp_term * trig_term_x, exp_term * trig_term_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot of the above field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth problem specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_divisions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_position_2d(data):\n",
    "    # introduce alternating z label\n",
    "    n_points = len(data[0])\n",
    "    label = jnp.tile(jnp.array([0.0, 1.0]), n_points)\n",
    "    return jnp.vstack((jnp.repeat(data, repeats=2, axis=1), label)).T\n",
    "\n",
    "def stack_vector(data):\n",
    "    return data.T.flatten().reshape(-1, 1)\n",
    "\n",
    "def dataset_3d(pos, obs):\n",
    "    return gpx.Dataset(label_position_2d(pos), stack_vector(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = jnp.mgrid[0:4:n_divisions*1j, 0:4:n_divisions*1j].reshape(2, -1)\n",
    "\n",
    "observations = jnp.stack(\n",
    "    div_free_2d_example(positions[0], positions[1]), axis=0\n",
    ").reshape(2, -1)\n",
    "\n",
    "dataset = dataset_3d(positions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_key = jr.PRNGKey(0)\n",
    "\n",
    "def train_test_split(positions, observations, n_train, n_split, key=default_key):\n",
    "    permutation = jr.permutation(key, jnp.arange(positions.shape[1]))\n",
    "    train_indices = permutation[:n_train]\n",
    "    test_indices = permutation[n_train:n_train+n_split]  # fmt: skip\n",
    "\n",
    "    n_dims = positions.shape[0]\n",
    "\n",
    "    match n_dims:\n",
    "        case 2:\n",
    "            dataset_train = dataset_3d(\n",
    "                positions[:, train_indices], observations[:, train_indices]\n",
    "            )\n",
    "            dataset_test = dataset_3d(\n",
    "                positions[:, test_indices], observations[:, test_indices]\n",
    "            )\n",
    "        case 3:\n",
    "            # CR TODO: implement me\n",
    "            assert False\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid number of dimensions: {n_dims}\")\n",
    "\n",
    "    return dataset_train, dataset_test, positions[:, test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One choice of prior distribution is a Gaussian Process with a diagonal kernel. This is where:\n",
    "$$\\mathbf f \\sim \\mathcal{GP}(\\mathbf 0,k_g\\mathbf I)$$\n",
    "such that the outputs of the predicted function are independent. This is equivalent to totally separating the dataset into two different datasets: $\\mathcal D_1:=\\{(\\mathbf x_i, y^{(1)}_i)\\}_{i=0}^{400}$ and $\\mathcal D_2:=\\{(\\mathbf x_i, y^{(2)}_i)\\}_{i=0}^{400}$ and performing a Gaussian Process Regression on each dataset separately.\n",
    "\n",
    "This does not use any prior information about the divergence-free nature of the underlying latent function, so is expected to perform worse than other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VectorKernel_2d(gpx.kernels.AbstractKernel):\n",
    "\n",
    "    # CR TODO: should this allow specification of lengthscale and variance?\n",
    "    kernel: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1])\n",
    "\n",
    "    def __call__(\n",
    "        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        # standard RBF-SE kernel if x and x' are on the same output, otherwise returns 0\n",
    "\n",
    "        w = jnp.array(X[2], dtype=int)\n",
    "        wp = jnp.array(Xp[2], dtype=int)\n",
    "\n",
    "        # drop output label to reduce resource usage\n",
    "        X = X[:2]\n",
    "        Xp = Xp[:2]\n",
    "\n",
    "        K = (w == wp) * self.kernel(X, Xp)\n",
    "\n",
    "        return K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPJax implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_gp(kernel, mean, dataset):\n",
    "    prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n",
    "    likelihood = gpx.likelihoods.Gaussian(\n",
    "        num_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)\n",
    "    )\n",
    "    posterior = prior * likelihood\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_key = jr.PRNGKey(0)\n",
    "\n",
    "training_data, _, _ = train_test_split(\n",
    "    positions, observations, 50, 1, key=simulation_key\n",
    ")\n",
    "\n",
    "\n",
    "mean = gpx.mean_functions.Zero()\n",
    "kernel = VectorKernel_2d()\n",
    "diagonal_posterior = initialise_gp(kernel, mean, training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_mll(posterior, dataset, NIters=1000):\n",
    "    # define the MLL using dataset_train\n",
    "    objective = gpx.objectives.ConjugateMLL(negative=True)\n",
    "    # Optimise to minimise the MLL\n",
    "    opt_posterior, _ = gpx.fit_scipy(\n",
    "        model=posterior,\n",
    "        objective=objective,\n",
    "        train_data=dataset,\n",
    "        max_iters=NIters,\n",
    "    )\n",
    "    return opt_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_diagonal_posterior = optimise_mll(diagonal_posterior, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are evaluated by calculating the RMSE between the predicted and true outputs. In particular,\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{800}\\sum_{i=1}^{400} \\|\\mathbf y_p-\\mathbf y_t\\|_2^2}$$\n",
    "\n",
    "where $\\mathbf y_p$ are the predicted values and $\\mathbf y_t$ are the true values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field + residuals plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, truth):\n",
    "    # in the paper they compute RMS per vectror component\n",
    "    return jnp.sqrt(jnp.sum((predictions - truth) ** 2) / truth.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_distribution(opt_posterior, prediction_locations, dataset_train):\n",
    "    latent = opt_posterior.predict(prediction_locations, train_data=dataset_train)\n",
    "    latent_mean = latent.mean()\n",
    "    latent_std = latent.stddev()\n",
    "    return latent_mean, latent_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal_mean, diagonal_std = latent_distribution(\n",
    "    opt_diagonal_posterior, dataset.X, training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_diagonal = dataset_3d(positions, diagonal_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(dataset_latent_diagonal.y, dataset.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergence free kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of divergence free kernel\n",
    "The latent function $\\mathbf f$ was chosen such that it is divergence-free, in other words:\n",
    "$$\\boldsymbol \\nabla\\cdot \\mathbf f := \\dfrac{\\partial f_1}{\\partial x_1} + \\dfrac{\\partial f_2}{\\partial x_2}=0$$\n",
    "\n",
    "Note that*, given any differentiable function $\\mathbf g$, the function:\n",
    "$$\\mathbf f:= \\mathscr G_\\mathbf x \\mathbf g := \\begin{pmatrix}-\\dfrac{\\partial}{\\partial x_2}\\\\ \\dfrac{\\partial}{\\partial x_1}\\end{pmatrix} \\mathbf g$$\n",
    "automatically satisfies the required constraint.\n",
    "\n",
    "As is the case with multivariate Gaussians, linear transformations of GPs are GPs (and they transform in much the same way).\n",
    "In particular:\n",
    "$$\\mathbf g \\sim \\mathcal{GP}(\\mathbf 0, K) \\implies \\mathscr G_\\mathbf x \\mathbf g \\sim \\mathcal{GP}(\\mathbf 0, \\mathscr G_\\mathbf x K \\mathscr G_{\\mathbf x'}^\\top)$$\n",
    "\n",
    "In our case, we choose $\\mathbf g \\sim \\mathcal{GP}(\\mathbf 0, k_g \\mathbf I)$ where $k_g$ is the squared exponential kernel. Therefore, any $\\mathbf f$ picked from the distribution $\\mathbf f \\sim \\mathcal {GP}(\\mathbf 0, \\mathscr G_\\mathbf x  k_g(\\mathbf x, \\mathbf x') \\mathscr G_{\\mathbf x'}^\\top)$, where\n",
    "$$\\mathscr G_\\mathbf x  k_g(\\mathbf x, \\mathbf x') \\mathscr G_{\\mathbf x'}^\\top= \\begin{pmatrix}\\dfrac{\\partial^2}{\\partial x_2 x_2'} & -\\dfrac{\\partial^2}{\\partial x_2 x_1'}\\\\-\\dfrac{\\partial^2}{\\partial x_1 x_2'} & \\dfrac{\\partial^2}{\\partial x_1 x_1'}\\end{pmatrix}k_g(\\mathbf x, \\mathbf x')$$\n",
    "will satisfy the required constraint. This is the divergence-free kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Much of the paper is dedicated to devising a systematic way to construct $\\mathscr{G}_x$ for arbitrary linear constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_hessian(\n",
    "    kernel, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    ") -> Float[Array, \"1\"]:\n",
    "    # compute all relevant second derivatives at once\n",
    "    # eg small_hessian(k)[0][1] is d2k/dx1dy2\n",
    "    return jnp.array(\n",
    "        jacfwd(jacrev(kernel, argnums=0), argnums=1)(X, Xp), dtype=jnp.float64\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DivFreeKernel(gpx.kernels.AbstractKernel):\n",
    "    kernel: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1])\n",
    "\n",
    "    def __call__(\n",
    "        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        # the third dimension switches between 00, 01 and 11 kernels\n",
    "\n",
    "        z = jnp.array(X[2], dtype=int)\n",
    "        zp = jnp.array(Xp[2], dtype=int)\n",
    "\n",
    "        # achieve the correct value via 'switches' that are either 1 or 0\n",
    "        k00_switch = ((z + 1) % 2) * ((zp + 1) % 2)\n",
    "        k01_switch = ((z + 1) % 2) * zp\n",
    "        k10_switch = z * ((zp + 1) % 2)\n",
    "        k11_switch = z * zp\n",
    "\n",
    "        # drop output label to reduce resource usage\n",
    "        X = jnp.array(X[0:2])\n",
    "        Xp = jnp.array(Xp[0:2])\n",
    "\n",
    "        hess = small_hessian(self.kernel, X, Xp)\n",
    "\n",
    "        K = (\n",
    "            k00_switch * hess[1][1]\n",
    "            - k01_switch * hess[1][0]\n",
    "            - k10_switch * hess[0][1]\n",
    "            + k11_switch * hess[0][0]\n",
    "        )\n",
    "\n",
    "        return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = DivFreeKernel()\n",
    "div_free_posterior = initialise_gp(kernel, mean, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_div_free_posterior = optimise_mll(div_free_posterior, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_free_mean, div_free_std = latent_distribution(\n",
    "    opt_div_free_posterior, dataset.X, training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_div_free = dataset_3d(positions, div_free_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(dataset_latent_div_free.y, dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure testing data alternates between x0 and x1 components\n",
    "def nlpd(mean, std, test_positions):\n",
    "    test_grid = jnp.column_stack((test_positions[0], test_positions[1])).flatten()\n",
    "    normal = tfp.substrates.jax.distributions.Normal(loc=mean, scale=std)\n",
    "    return -jnp.sum(normal.log_prob(test_grid))\n",
    "\n",
    "\n",
    "# compute nlpd for velocity and helmholtz\n",
    "nlpd_diagonal = nlpd(diagonal_mean, diagonal_std, positions)\n",
    "nlpd_div_free = nlpd(div_free_mean, div_free_std, positions)\n",
    "\n",
    "print(f\"NLPD for diagonal: {nlpd_diagonal:.2E} \\nNLPD for div free: {nlpd_div_free:.2E}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
